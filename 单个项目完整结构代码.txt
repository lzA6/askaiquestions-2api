é¡¹ç›® 'askaiquestions-2api' çš„ç»“æ„æ ‘:
ğŸ“‚ askaiquestions-2api/
    ğŸ“„ .env
    ğŸ“„ .env.example
    ğŸ“„ Dockerfile
    ğŸ“„ docker-compose.yml
    ğŸ“„ main.py
    ğŸ“„ nginx.conf
    ğŸ“„ requirements.txt
    ğŸ“‚ app/
        ğŸ“‚ core/
            ğŸ“„ __init__.py
            ğŸ“„ config.py
        ğŸ“‚ providers/
            ğŸ“„ __init__.py
            ğŸ“„ askai_provider.py
            ğŸ“„ base_provider.py
        ğŸ“‚ utils/
            ğŸ“„ sse_utils.py
================================================================================

--- æ–‡ä»¶è·¯å¾„: .env ---

# [è‡ªåŠ¨å¡«å……] askaiquestions-2api ç”Ÿäº§ç¯å¢ƒé…ç½®
# è¯¥æ–‡ä»¶ç”± Project Chimera è‡ªåŠ¨ç”Ÿæˆï¼Œå¯ç›´æ¥ç”¨äºä¸€é”®éƒ¨ç½²ã€‚

# --- å®‰å…¨é…ç½® ---
# ç”¨äºä¿æŠ¤æ‚¨çš„ API æœåŠ¡çš„è®¿é—®å¯†é’¥ï¼Œè¯·æŒ‰éœ€ä¿®æ”¹ä¸ºæ‚¨è‡ªå·±çš„å¤æ‚å¯†é’¥ã€‚
API_MASTER_KEY=1

# --- ç«¯å£é…ç½® ---
# Nginx å¯¹å¤–æš´éœ²çš„ç«¯å£
NGINX_PORT=8088


--- æ–‡ä»¶è·¯å¾„: .env.example ---

# ====================================================================
# askaiquestions-2api é…ç½®æ–‡ä»¶æ¨¡æ¿
# ====================================================================
#
# è¯·å°†æ­¤æ–‡ä»¶é‡å‘½åä¸º ".env" å¹¶å¡«å…¥æ‚¨çš„é…ç½®ã€‚
#

# --- æ ¸å¿ƒå®‰å…¨é…ç½® (å¿…é¡»è®¾ç½®) ---
# ç”¨äºä¿æŠ¤æ‚¨ API æœåŠ¡çš„è®¿é—®å¯†é’¥ã€‚
API_MASTER_KEY=sk-askai-default-key-please-change-me

# --- éƒ¨ç½²é…ç½® (å¯é€‰) ---
# Nginx å¯¹å¤–æš´éœ²çš„ç«¯å£
NGINX_PORT=8088


--- æ–‡ä»¶è·¯å¾„: Dockerfile ---

# ====================================================================
# Dockerfile for askaiquestions-2api (v1.0 - High-Speed Pseudo-Stream)
# ====================================================================

FROM python:3.10-slim

# è®¾ç½®ç¯å¢ƒå˜é‡
ENV PYTHONDONTWRITEBYTECODE=1
ENV PYTHONUNBUFFERED=1
WORKDIR /app

# å®‰è£… Python ä¾èµ–
COPY requirements.txt .
RUN pip install --no-cache-dir --upgrade pip && \
    pip install --no-cache-dir -r requirements.txt

# å¤åˆ¶åº”ç”¨ä»£ç 
COPY . .

# åˆ›å»ºå¹¶åˆ‡æ¢åˆ°é root ç”¨æˆ·
RUN useradd --create-home appuser && \
    chown -R appuser:appuser /app
USER appuser

# æš´éœ²ç«¯å£å¹¶å¯åŠ¨ (ä½¿ç”¨å¤šä¸ª worker ä»¥å¤„ç†å¹¶å‘è¯·æ±‚)
EXPOSE 8000
CMD ["uvicorn", "main:app", "--host", "0.0.0.0", "--port", "8000", "--workers", "1"]


--- æ–‡ä»¶è·¯å¾„: docker-compose.yml ---

services:
  nginx:
    image: nginx:latest
    container_name: askaiquestions-2api-nginx
    restart: always
    ports:
      - "${NGINX_PORT:-8088}:80"
    volumes:
      - ./nginx.conf:/etc/nginx/nginx.conf:ro
    depends_on:
      - app
    networks:
      - askai-net

  app:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: askaiquestions-2api-app
    restart: unless-stopped
    env_file:
      - .env
    networks:
      - askai-net

networks:
  askai-net:
    driver: bridge


--- æ–‡ä»¶è·¯å¾„: main.py ---

import logging
from contextlib import asynccontextmanager
from typing import Optional

from fastapi import FastAPI, Request, HTTPException, Depends, Header
from fastapi.responses import JSONResponse, StreamingResponse

from app.core.config import settings
from app.providers.askai_provider import AskAIProvider

# --- æ—¥å¿—é…ç½® ---
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# --- å…¨å±€ Provider å®ä¾‹ ---
provider = AskAIProvider()

@asynccontextmanager
async def lifespan(app: FastAPI):
    logger.info(f"åº”ç”¨å¯åŠ¨ä¸­... {settings.APP_NAME} v{settings.APP_VERSION}")
    logger.info("æœåŠ¡å·²è¿›å…¥ 'é«˜é€Ÿä¼ªæµ' æ¨¡å¼ã€‚")
    logger.info(f"æœåŠ¡å°†åœ¨ http://localhost:{settings.NGINX_PORT} ä¸Šå¯ç”¨")
    yield
    logger.info("åº”ç”¨å…³é—­ã€‚")

app = FastAPI(
    title=settings.APP_NAME,
    version=settings.APP_VERSION,
    description=settings.DESCRIPTION,
    lifespan=lifespan
)

# --- å®‰å…¨ä¾èµ– ---
async def verify_api_key(authorization: Optional[str] = Header(None)):
    if settings.API_MASTER_KEY and settings.API_MASTER_KEY != "1":
        if not authorization or "bearer" not in authorization.lower():
            raise HTTPException(status_code=401, detail="éœ€è¦ Bearer Token è®¤è¯ã€‚")
        token = authorization.split(" ")[-1]
        if token != settings.API_MASTER_KEY:
            raise HTTPException(status_code=403, detail="æ— æ•ˆçš„ API Keyã€‚")

# --- API è·¯ç”± ---
@app.post("/v1/chat/completions", dependencies=[Depends(verify_api_key)])
async def chat_completions(request: Request):
    try:
        request_data = await request.json()
        # æ£€æŸ¥æ˜¯å¦æ˜¯æµå¼è¯·æ±‚
        is_stream = request_data.get("stream", False)
        return await provider.chat_completion(request_data, is_stream)
    except Exception as e:
        logger.error(f"å¤„ç†èŠå¤©è¯·æ±‚æ—¶å‘ç”Ÿé¡¶å±‚é”™è¯¯: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"å†…éƒ¨æœåŠ¡å™¨é”™è¯¯: {str(e)}")

@app.get("/v1/models", dependencies=[Depends(verify_api_key)], response_class=JSONResponse)
async def list_models():
    return await provider.get_models()

@app.get("/", summary="æ ¹è·¯å¾„", include_in_schema=False)
def root():
    return {"message": f"æ¬¢è¿æ¥åˆ° {settings.APP_NAME} v{settings.APP_VERSION}. æœåŠ¡è¿è¡Œæ­£å¸¸ã€‚"}


--- æ–‡ä»¶è·¯å¾„: nginx.conf ---

worker_processes auto;

events {
    worker_connections 1024;
}

http {
    upstream askai_backend {
        server app:8000;
    }

    server {
        listen 80;
        server_name localhost;

        location / {
            proxy_pass http://askai_backend;
            proxy_set_header Host $host;
            proxy_set_header X-Real-IP $remote_addr;
            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
            proxy_set_header X-Forwarded-Proto $scheme;
            
            # æµå¼ä¼ è¾“ä¼˜åŒ–
            proxy_buffering off;
            proxy_cache off;
            proxy_set_header Connection '';
            proxy_http_version 1.1;
            chunked_transfer_encoding off;
        }
    }
}


--- æ–‡ä»¶è·¯å¾„: requirements.txt ---

fastapi
uvicorn[standard]
httpx
pydantic-settings
python-dotenv


--- æ–‡ä»¶è·¯å¾„: app\core\__init__.py ---



--- æ–‡ä»¶è·¯å¾„: app\core\config.py ---

from pydantic_settings import BaseSettings, SettingsConfigDict
from typing import List, Optional

class Settings(BaseSettings):
    model_config = SettingsConfigDict(
        env_file=".env",
        env_file_encoding='utf-8',
        extra="ignore"
    )

    APP_NAME: str = "askaiquestions-2api"
    APP_VERSION: str = "1.0.0"
    DESCRIPTION: str = "ä¸€ä¸ªå°† askaiquestions.net è½¬æ¢ä¸ºå…¼å®¹ OpenAI æ ¼å¼ API çš„é«˜æ€§èƒ½ä»£ç†ã€‚"

    API_MASTER_KEY: Optional[str] = None
    
    API_REQUEST_TIMEOUT: int = 120
    NGINX_PORT: int = 8088

    DEFAULT_MODEL: str = "askai-default-model"
    KNOWN_MODELS: List[str] = ["askai-default-model"]

settings = Settings()


--- æ–‡ä»¶è·¯å¾„: app\providers\__init__.py ---



--- æ–‡ä»¶è·¯å¾„: app\providers\askai_provider.py ---

import httpx
import json
import time
import logging
import uuid
import asyncio
from typing import Dict, Any, AsyncGenerator, Union

from fastapi import HTTPException
from fastapi.responses import StreamingResponse, JSONResponse

from app.core.config import settings
from app.providers.base_provider import BaseProvider
from app.utils.sse_utils import (
    create_sse_data, 
    create_chat_completion_chunk, 
    create_non_stream_chat_completion,
    DONE_CHUNK
)

logger = logging.getLogger(__name__)

class AskAIProvider(BaseProvider):
    def __init__(self):
        self.client = httpx.AsyncClient(timeout=settings.API_REQUEST_TIMEOUT)
        self.api_url = "https://pjfuothbq9.execute-api.us-east-1.amazonaws.com/get-summary"

    async def chat_completion(self, request_data: Dict[str, Any], stream: bool) -> Union[StreamingResponse, JSONResponse]:
        
        full_text, request_id, model = await self._get_upstream_response(request_data)

        if stream:
            return StreamingResponse(
                self._stream_generator(full_text, request_id, model), 
                media_type="text/event-stream"
            )
        else:
            completion_data = create_non_stream_chat_completion(request_id, model, full_text)
            return JSONResponse(content=completion_data)

    async def _get_upstream_response(self, request_data: Dict[str, Any]) -> (str, str, str):
        """
        ç»Ÿä¸€å¤„ç†ä¸Šæ¸¸è¯·æ±‚ï¼Œè¿”å› (å“åº”æ–‡æœ¬, è¯·æ±‚ID, æ¨¡å‹åç§°)ã€‚
        """
        request_id = f"chatcmpl-{uuid.uuid4()}"
        model = request_data.get("model", settings.DEFAULT_MODEL)
        
        try:
            payload = self._prepare_payload(request_data)
            headers = self._prepare_headers()

            response = await self.client.post(self.api_url, headers=headers, json=payload)
            response.raise_for_status()
            
            # åº”ç”¨ã€æ¨¡å¼ï¼šJSON-in-Text-Parsingã€‘
            response_text = response.text
            data = json.loads(response_text)
            
            summary = data.get("summary")
            if summary is None:
                raise ValueError("ä¸Šæ¸¸å“åº”çš„ JSON ä¸­ç¼ºå°‘ 'summary' å­—æ®µã€‚")
            
            return summary, request_id, model

        except httpx.HTTPStatusError as e:
            logger.error(f"è¯·æ±‚ä¸Šæ¸¸ API æ—¶å‘ç”Ÿ HTTP é”™è¯¯: {e.response.status_code} - {e.response.text}", exc_info=True)
            raise HTTPException(status_code=e.response.status_code, detail=f"ä¸Šæ¸¸æœåŠ¡é”™è¯¯: {e.response.text}")
        except (json.JSONDecodeError, ValueError) as e:
            logger.error(f"è§£æä¸Šæ¸¸å“åº”æ—¶å‡ºé”™: {e}", exc_info=True)
            raise HTTPException(status_code=502, detail=f"æ— æ³•è§£æä¸Šæ¸¸æœåŠ¡å“åº”: {e}")
        except Exception as e:
            logger.error(f"è¯·æ±‚ä¸Šæ¸¸æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}", exc_info=True)
            raise HTTPException(status_code=500, detail=f"å†…éƒ¨æœåŠ¡å™¨é”™è¯¯: {e}")

    async def _stream_generator(self, full_text: str, request_id: str, model: str) -> AsyncGenerator[bytes, None]:
        """
        åº”ç”¨ã€æ¨¡å¼ï¼šé«˜é€Ÿåˆ†å—ä¼ªæµã€‘
        """
        try:
            # ä¸ºäº†è¾¾åˆ° 1000 tokens/s çš„æ„ŸçŸ¥é€Ÿåº¦ï¼Œæˆ‘ä»¬æ¯æ¬¡å‘é€2ä¸ªå­—ç¬¦ï¼Œå»¶è¿Ÿ1æ¯«ç§’
            chunk_size = 2
            delay = 0.001 
            
            for i in range(0, len(full_text), chunk_size):
                chunk_content = full_text[i:i+chunk_size]
                chunk = create_chat_completion_chunk(request_id, model, chunk_content)
                yield create_sse_data(chunk)
                await asyncio.sleep(delay)
            
            # å‘é€ç»“æŸæ ‡å¿—
            final_chunk = create_chat_completion_chunk(request_id, model, "", "stop")
            yield create_sse_data(final_chunk)
            yield DONE_CHUNK
        except Exception as e:
            logger.error(f"æµç”Ÿæˆå™¨å‘ç”Ÿé”™è¯¯: {e}", exc_info=True)
            error_chunk = create_chat_completion_chunk(request_id, model, f"å†…éƒ¨é”™è¯¯: {e}", "stop")
            yield create_sse_data(error_chunk)
            yield DONE_CHUNK

    def _prepare_headers(self) -> Dict[str, str]:
        return {
            "accept": "*/*",
            "accept-language": "zh-CN,zh;q=0.9,en;q=0.8",
            "content-type": "application/json",
            "origin": "https://askaiquestions.net",
            "referer": "https://askaiquestions.net/",
            "user-agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/141.0.0.0 Safari/537.36"
        }

    def _prepare_payload(self, request_data: Dict[str, Any]) -> Dict[str, Any]:
        messages = request_data.get("messages", [])
        if not messages:
            raise HTTPException(status_code=400, detail="è¯·æ±‚ä½“ä¸­ç¼ºå°‘ 'messages' å­—æ®µã€‚")
        
        return {
            "website": "ask-ai-questions",
            "messages": messages
        }

    async def get_models(self) -> JSONResponse:
        model_data = {
            "object": "list",
            "data": [
                {"id": name, "object": "model", "created": int(time.time()), "owned_by": "lzA6"}
                for name in settings.KNOWN_MODELS
            ]
        }
        return JSONResponse(content=model_data)


--- æ–‡ä»¶è·¯å¾„: app\providers\base_provider.py ---

from abc import ABC, abstractmethod
from typing import Dict, Any, Union
from fastapi.responses import StreamingResponse, JSONResponse

class BaseProvider(ABC):
    @abstractmethod
    async def chat_completion(
        self,
        request_data: Dict[str, Any],
        stream: bool
    ) -> Union[StreamingResponse, JSONResponse]:
        pass

    @abstractmethod
    async def get_models(self) -> JSONResponse:
        pass


--- æ–‡ä»¶è·¯å¾„: app\utils\sse_utils.py ---

import json
import time
from typing import Dict, Any, Optional

DONE_CHUNK = b"data: [DONE]\n\n"

def create_sse_data(data: Dict[str, Any]) -> bytes:
    """å°†å­—å…¸æ•°æ®æ ¼å¼åŒ–ä¸º SSE äº‹ä»¶å­—ç¬¦ä¸²ã€‚"""
    return f"data: {json.dumps(data)}\n\n".encode('utf-8')

def create_chat_completion_chunk(
    request_id: str,
    model: str,
    content: str,
    finish_reason: Optional[str] = None
) -> Dict[str, Any]:
    """åˆ›å»ºä¸€ä¸ªä¸ OpenAI å…¼å®¹çš„èŠå¤©è¡¥å…¨æµå¼å—ã€‚"""
    return {
        "id": request_id,
        "object": "chat.completion.chunk",
        "created": int(time.time()),
        "model": model,
        "choices": [
            {
                "index": 0,
                "delta": {"content": content},
                "finish_reason": finish_reason
            }
        ]
    }

def create_non_stream_chat_completion(
    request_id: str,
    model: str,
    content: str
) -> Dict[str, Any]:
    """åˆ›å»ºä¸€ä¸ªä¸ OpenAI å…¼å®¹çš„éæµå¼èŠå¤©è¡¥å…¨å“åº”ã€‚"""
    return {
        "id": request_id,
        "object": "chat.completion",
        "created": int(time.time()),
        "model": model,
        "choices": [
            {
                "index": 0,
                "message": {
                    "role": "assistant",
                    "content": content,
                },
                "finish_reason": "stop",
            }
        ],
        "usage": {
            "prompt_tokens": 0,
            "completion_tokens": 0,
            "total_tokens": 0,
        },
    }



